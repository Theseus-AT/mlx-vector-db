# ğŸ MLX Vector Database

A high-performance vector database optimized for Apple Silicon, built with MLX for lightning-fast similarity search and RAG applications.

![MLX](https://img.shields.io/badge/MLX-0.25.2-green)
![Python](https://img.shields.io/badge/Python-3.9+-blue)
![Apple Silicon](https://img.shields.io/badge/Apple%20Silicon-Optimized-orange)
![License](https://img.shields.io/badge/License-MIT-yellow)

## ğŸš€ Features

- **ğŸ Native Apple Silicon Support** - Optimized for M1/M2/M3 chips with MLX
- **âš¡ Lightning Fast Performance** - 250K+ vectors/sec, sub-millisecond queries
- **ğŸ§  MLX-LM Integration** - Built-in text embedding and RAG pipeline
- **ğŸ”§ Production Ready** - Rate limiting, monitoring, error handling
- **ğŸ“¡ RESTful API** - FastAPI with comprehensive SDK
- **ğŸ¯ Type Safe** - Full TypeScript-style type annotations
- **ğŸ” Advanced Search** - Metadata filtering, batch queries, similarity search
- **ğŸ“Š Real-time Monitoring** - Performance metrics and health checks

## ğŸ“‹ Prerequisites

### System Requirements
- **macOS** with Apple Silicon (M1/M2/M3)
- **Python 3.9+**
- **8GB+ RAM** recommended
- **Xcode Command Line Tools**

### Quick Setup Check
```bash
# Verify Apple Silicon
uname -m  # Should output: arm64

# Check Python version
python3 --version  # Should be 3.9+
```

## ğŸ› ï¸ Installation

### 1. Clone Repository
```bash
git clone <your-repo-url>
cd mlx-vector-database
```

### 2. Install Dependencies
```bash
# Core dependencies
pip install -r requirements.txt

# Optional: For advanced MLX-LM features
pip install mlx-lm sentence-transformers
```

### 3. Environment Setup
```bash
# Copy environment template
cp .env.example .env

# Edit with your settings
nano .env
```

### Required Environment Variables
```bash
# API Configuration
VECTOR_DB_API_KEY=mlx-vector-dev-key-2024
VECTOR_DB_ADMIN_KEY=mlx-vector-admin-key-2024

# Server Settings
HOST=localhost
PORT=8000
ENVIRONMENT=development

# Optional: Advanced Features
ENABLE_METRICS=true
LOG_LEVEL=INFO
MAX_VECTORS_PER_STORE=1000000
RATE_LIMIT_REQUESTS=100
```

## ğŸš€ Quick Start

### 1. Start the Server
```bash
python main.py
```

Expected output:
```
ğŸ MLX Vector Database Server
âœ… MLX System Check: Device(gpu, 0)
ğŸš€ Server running on http://localhost:8000
```

### 2. Verify Installation
```bash
# Run health check
curl http://localhost:8000/health

# Expected response:
{
  "status": "healthy",
  "mlx_device": "Device(gpu, 0)",
  "version": "1.0.0"
}
```

### 3. Run Demo Tests
```bash
# Basic functionality
python demo.py

# Complete integration test
python sprint3_demo.py

# Quick validation
python simple_test.py
```

## ğŸ’» Usage Examples

### Python SDK

#### Basic Vector Operations
```python
from mlx_vector_client import MLXVectorClient
import numpy as np

# Initialize client
client = MLXVectorClient("http://localhost:8000", "mlx-vector-dev-key-2024")

# Create store
await client.create_store("user123", "embeddings")

# Add vectors
vectors = np.random.random((10, 384)).astype(np.float32)
metadata = [{"id": f"doc_{i}", "category": "A"} for i in range(10)]
await client.add_vectors("user123", "embeddings", vectors, metadata)

# Query similar vectors
query_vector = np.random.random((384,)).astype(np.float32)
results = await client.query_vectors("user123", "embeddings", query_vector, k=5)

print(f"Found {len(results)} similar vectors")
```

#### MLX-LM Text Pipeline
```python
from mlx_lm_integration import MLXTextEmbeddingPipeline

# Initialize pipeline
pipeline = MLXTextEmbeddingPipeline()

# Index documents
documents = [
    "Apple Silicon provides exceptional ML performance",
    "Vector databases enable semantic search",
    "MLX framework optimizes Apple hardware"
]

await pipeline.index_documents(documents, "user123", "knowledge_base")

# Semantic search
results = await pipeline.search(
    "How does Apple Silicon help with AI?",
    "user123", "knowledge_base",
    k=2
)

for result in results:
    print(f"Similarity: {result.similarity:.3f}")
    print(f"Text: {result.text}")
```

#### Context Manager (Recommended)
```python
async with MLXVectorClient("http://localhost:8000", api_key) as client:
    # Automatic connection management
    await client.add_vectors("user123", "model", vectors, metadata)
    results = await client.query_vectors("user123", "model", query_vector)
    # Automatic cleanup
```

### REST API

#### Authentication
All requests require API key in header:
```bash
curl -H "X-API-Key: mlx-vector-dev-key-2024" \
     http://localhost:8000/health
```

#### Create Vector Store
```bash
curl -X POST "http://localhost:8000/admin/create_store" \
     -H "X-API-Key: mlx-vector-dev-key-2024" \
     -H "Content-Type: application/json" \
     -d '{
       "user_id": "user123",
       "model_id": "embeddings",
       "dimension": 384,
       "metric": "cosine"
     }'
```

#### Add Vectors
```bash
curl -X POST "http://localhost:8000/vectors/add" \
     -H "X-API-Key: mlx-vector-dev-key-2024" \
     -H "Content-Type: application/json" \
     -d '{
       "user_id": "user123",
       "model_id": "embeddings",
       "vectors": [[0.1, 0.2, 0.3, ...]],
       "metadata": [{"id": "doc_1", "category": "A"}]
     }'
```

#### Query Vectors
```bash
curl -X POST "http://localhost:8000/vectors/query" \
     -H "X-API-Key: mlx-vector-dev-key-2024" \
     -H "Content-Type: application/json" \
     -d '{
       "user_id": "user123",
       "model_id": "embeddings",
       "query_vector": [0.1, 0.2, 0.3, ...],
       "k": 5,
       "filters": {"category": "A"}
     }'
```

## ğŸ”§ Configuration

### Performance Tuning
```python
# config.py
PERFORMANCE_CONFIG = {
    "max_vectors_per_store": 1000000,
    "batch_size": 1000,
    "mlx_compile_cache": True,
    "memory_map_threshold": 100000,
    "parallel_queries": True
}
```

### Store Configuration
```python
# Different distance metrics
store_configs = {
    "cosine": {"metric": "cosine"},      # Default, best for embeddings
    "euclidean": {"metric": "l2"},       # Good for image features
    "manhattan": {"metric": "l1"}        # Good for sparse vectors
}
```

### Advanced MLX Settings
```python
# MLX optimization
import mlx.core as mx

# Set memory pool (optional)
mx.set_memory_pool_size(1024 * 1024 * 1024)  # 1GB

# Enable compilation cache
mx.set_cache_directory("/tmp/mlx_cache")
```

## ğŸ“Š Monitoring & Health Checks

### Health Endpoints
```bash
# Basic health
GET /health

# Performance health  
GET /vectors/health

# Store statistics
GET /admin/store/stats?user_id=user123&model_id=embeddings
```

### Performance Metrics
```python
# Get performance stats
stats = await client.get_store_stats("user123", "embeddings")
print(f"QPS: {stats['queries_per_second']}")
print(f"Latency: {stats['avg_query_time_ms']}ms")
print(f"Memory: {stats['memory_usage_mb']}MB")
```

### Monitoring Dashboard
Access metrics at: `http://localhost:8000/metrics` (if enabled)

## ğŸš¨ Troubleshooting

### Common Issues

#### MLX Device Not Found
```bash
# Check MLX installation
python -c "import mlx.core as mx; print(mx.default_device())"

# Should output: Device(gpu, 0)
# If not, reinstall MLX: pip install --upgrade mlx
```

#### Memory Issues
```python
# Optimize memory usage
client.config.batch_size = 100  # Reduce batch size
client.config.max_cache_size = 1000  # Limit cache
```

#### Performance Issues
```python
# Enable MLX compilation
client.config.enable_compilation = True

# Warm up the system
await client.warmup_kernels()
```

#### Connection Issues
```bash
# Check server status
curl http://localhost:8000/health

# Verify API key
export VECTOR_DB_API_KEY=your-key-here
```

### Debug Mode
```bash
# Run with debug logging
LOG_LEVEL=DEBUG python main.py

# Enable MLX debug
MLX_DEBUG=1 python main.py
```

## ğŸ”’ Security Best Practices

### API Key Management
```bash
# Use environment variables
export VECTOR_DB_API_KEY=your-secure-key

# Or use key file
echo "your-secure-key" > .api_key
chmod 600 .api_key
```

### Production Deployment
```bash
# Use HTTPS in production
ENABLE_HTTPS=true
SSL_CERT_PATH=/path/to/cert.pem
SSL_KEY_PATH=/path/to/key.pem

# Enable rate limiting
RATE_LIMIT_ENABLED=true
RATE_LIMIT_REQUESTS=100
RATE_LIMIT_WINDOW=60
```

### Access Control
```python
# Implement user isolation
store_id = f"user_{user_id}_{model_id}"

# Validate user permissions
if not user_can_access_store(user_id, store_id):
    raise PermissionError("Access denied")
```

## ğŸ“ˆ Performance Benchmarks

### Typical Performance (Apple M2 Pro)
- **Vector Addition**: 250,000+ vectors/second
- **Query Latency**: 0.4-0.8ms
- **Throughput**: 1,500+ queries/second
- **Memory Efficiency**: ~1MB per 10k vectors (384D)

### Scaling Guidelines
- **< 100k vectors**: Single store, excellent performance
- **100k - 1M vectors**: Consider sharding by user/topic
- **> 1M vectors**: Multi-store architecture recommended

## ğŸ§ª Testing

### Run All Tests
```bash
# Quick validation
python simple_test.py

# Complete test suite
python working_test_fixed.py

# Performance benchmarks
python demo.py

# Production readiness
python sprint3_demo.py
```

### Custom Tests
```python
# Write your own tests
import pytest
from mlx_vector_client import MLXVectorClient

@pytest.mark.asyncio
async def test_custom_functionality():
    client = MLXVectorClient("http://localhost:8000", "test-key")
    # Your test code here
```

## ğŸ¤ Contributing

### Development Setup
```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Pre-commit hooks
pre-commit install

# Run tests
pytest tests/
```

### Code Style
```bash
# Format code
black .
isort .

# Type checking
mypy src/
```

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Apple MLX Team** - For the amazing MLX framework
- **FastAPI** - For the excellent web framework
- **Sentence Transformers** - For embedding models

## ğŸ“ Support

- **Documentation**: [Wiki](wiki)
- **Issues**: [GitHub Issues](issues)
- **Discussions**: [GitHub Discussions](discussions)

---

**Made with â¤ï¸ for Apple Silicon developers**